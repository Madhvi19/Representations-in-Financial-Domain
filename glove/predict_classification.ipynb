{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "import concurrent\n",
    "import contractions\n",
    "import en_core_web_sm\n",
    "import logging as log\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import spacy\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from spacy import displacy\n",
    "from Stemmer import Stemmer\n",
    "from word2number import w2n\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "from word2number import w2n\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"$TSLA Maybe they calculate that 7ct energy price they get from Solar City panels:) if needed,so,all profit in Model 3 and Semi.Need more new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = pickle.load(open('model_pretrained_classification_gboost.pickle', 'rb'))\n",
    "model_our = pickle.load(open('model_our_classification_gboost.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __removePunctuations(sequence,ner_tags):\n",
    "    #########################################################################################\n",
    "    # This method removes any punctuations and gets only the text from the given sequence.\n",
    "    #########################################################################################\n",
    "    try:\n",
    "        if sequence is not None and sequence.strip() != \"\":\n",
    "            if sequence in ner_tags:\n",
    "                return re.sub('[^A-Za-z0-9%$.]+',' ',sequence)\n",
    "            else:\n",
    "                return re.sub('[^A-Za-z0-9$%]+',' ',sequence)\n",
    "        return sequence # return sequence as is without any changes\n",
    "    except:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        err = \"Error occurred while removing punctuations in the sequence '{0}'. Error is: {1}; {2}\".format(sequence, str(exc_type), str(exc_value))\n",
    "        raise Exception(err)\n",
    "def __clean_data(data,ner_tags):\n",
    "    #########################################################################################\n",
    "    # This method cleans the data by applying following API's and returns list of preprocessed tokens\n",
    "    #     data.lower()\n",
    "    #     __expandContractions(data)\n",
    "    #     __removeHtmlTags(data)\n",
    "    #     __replaceurls(data)\n",
    "    #     __removePunctuations(data)\n",
    "    #     data.split(\" \")\n",
    "    #########################################################################################\n",
    "    #data = __expandContractions(data)\n",
    "    #data = __removeHtmlTags(data) # remove tags\n",
    "    #data = __replaceurls(data)\n",
    "    data = data.replace(\"$\",\"$ \")\n",
    "    data = data.replace(\"%\",\" % \")\n",
    "    data = data.split(\" \")\n",
    "    clean = []\n",
    "    for word in data:\n",
    "        w_clean = __removePunctuations(word,ner_tags)\n",
    "        clean.extend(w_clean.split(\" \"))\n",
    "    data = clean\n",
    "    data = [word if word.isupper() and word.lower() in ner_tags else word.lower() for word in data]\n",
    "    dollar_list = ['$','k','%']\n",
    "    data = [d for d in data if len(d)>=2 or d in dollar_list]\n",
    "#     clean_list.append(data)\n",
    "    return data\n",
    "def __applyner(sequence):\n",
    "    #########################################################################################\n",
    "    # This method applies NER and returns the sequence according to the operation performed based on NER tag.\n",
    "    #########################################################################################\n",
    "    pickfile = open('tickermapping.pickle','rb')\n",
    "    tickermapping = pickle.load(pickfile)\n",
    "    ner_tags = []\n",
    "    doc = nlp(sequence)  # applying NER\n",
    "    for X in doc.ents:\n",
    "        # If the NER class is ORG\n",
    "        if X.label_ == 'ORG':\n",
    "            \"X.text can take microsoft corp or abcd name MSFT\"\n",
    "            text = X.text\n",
    "            if text in tickermapping.keys():\n",
    "                text = tickermapping[X.text]\n",
    "            text = re.sub(r'[^\\w\\s]', '', X.text).lower()\n",
    "            if 'inc' in text:\n",
    "                text = text.replace('inc', '')\n",
    "            if 'ltd' in text:\n",
    "                text = text.replace('ltd', '')\n",
    "            if 'llp' in text:\n",
    "                text = text.replace('llp', '')\n",
    "            if 'limited' in text:\n",
    "                text = text.replace('limited', '')\n",
    "            if 'corp' in text:\n",
    "                text = text.replace('corp', '')\n",
    "            if 'the' in text.lower():\n",
    "                text = text.replace('the','')\n",
    "            sequence = sequence.replace(X.text, text)\n",
    "            ner_tags.extend(text.lower().split(\" \"))\n",
    "        # If NER class is MONEY\n",
    "        if X.label_ == 'MONEY':\n",
    "            new_X = X.text.lower()\n",
    "            if 'approximately' in new_X:  # Remove all the words which might appear in NER money class\n",
    "                new_X = new_X.replace('approximately', '')\n",
    "            if 'per' in new_X:\n",
    "                new_X = new_X.replace('per', '')\n",
    "            if 'to' in new_X:\n",
    "                new_X = new_X.replace('to', '')\n",
    "            if 'and' in new_X:\n",
    "                new_X = new_X.replace('and', '')\n",
    "            if 'between' in new_X:\n",
    "                new_X = new_X.replace('between', '')\n",
    "            if 'phone' in new_X:\n",
    "                continue\n",
    "            # Apply NER for the string which is obtained after removing other words this gives $200, $500 as separate ones\n",
    "            if '$' not in new_X:\n",
    "                new_X = \"$\"+new_X\n",
    "            doc1 = nlp(new_X)\n",
    "            for Y in doc1.ents:\n",
    "                money = Y.text\n",
    "                if ' ' not in money:\n",
    "                    act_money = money.replace(',', '')  # Actual Money\n",
    "                    #act_money = act_money.replace('.','')\n",
    "                    sequence = sequence.replace(Y.text, act_money)  # Replace original money text with actual money\n",
    "                    ner_tags.append(act_money)\n",
    "                    # print(act_money)\n",
    "                else:\n",
    "                    money = Y.text[Y.text.find(\"$\") + 1:]\n",
    "                    k = money.find(' ')\n",
    "                    try:\n",
    "                        act_money = float(money[:k].replace(',', ''))\n",
    "                        #act_money = act_money.replace('.','')\n",
    "                        money_conv = w2n.word_to_num(money[k:])  # Conversion of word types million to *1e6\n",
    "                        sequence = sequence.replace(Y.text, \"$ \"+str(act_money * money_conv))  # Replace original money text with actual money\n",
    "                        #print(\"Converted from\", money, act_money * money_conv)\n",
    "                    except:\n",
    "                        continue  # if any exception dont modify the original sentence and continue\n",
    "        # If NER class is LAW\n",
    "        if X.label_ == 'LAW':\n",
    "            new_X = X.text\n",
    "            new_X = re.sub(r'[\\d.!?\\-\"]', '', new_X)\n",
    "            if 'the' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('the', '')\n",
    "            if 'of' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('of', '')\n",
    "            if 'section' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('section', '')\n",
    "            sequence = sequence.replace(X.text, new_X)\n",
    "            ner_tags.extend(new_X.split(\" \"))\n",
    "        # If NER class is Location\n",
    "        if X.label_ == 'GPE':\n",
    "            new_X = X.text.lower()\n",
    "            new_X = re.sub(r'[\\d.!?\\-\"]', '', new_X)\n",
    "            if 'the' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('the', '')\n",
    "            if '.' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('.', '')\n",
    "            sequence = sequence.replace(X.text, new_X)\n",
    "            ner_tags.extend(new_X.split(\" \"))\n",
    "        # If NER class is Person\n",
    "        if X.label_ == 'PERSON':\n",
    "            new_X = X.text.lower()\n",
    "            new_X = re.sub(r'[\\d.!?\\-\"]', '', new_X)\n",
    "            if 'the' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('the', '')\n",
    "            if '.' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('.', '')\n",
    "            sequence = sequence.replace(X.text, new_X)\n",
    "            ner_tags.extend(new_X.split(\" \"))\n",
    "        if X.label_ == 'CARDINAL':\n",
    "            number = X.text\n",
    "            number = number.replace(',','')\n",
    "            #number = number.replace('.','')\n",
    "            if number.isnumeric():\n",
    "                sequence = sequence.replace(X.text, number)\n",
    "        if X.label_ == 'QUANTITY':\n",
    "            quantity = X.text.split(\" \")\n",
    "            for number in quantity:\n",
    "                number = number.replace(',','')\n",
    "                number = number.replace('.','')\n",
    "                if number.isnumeric():\n",
    "                    sequence = sequence.replace(X.text, number)\n",
    "        if X.label_ == \"PERCENT\":\n",
    "            percent = X.text.replace('%','')\n",
    "            ner_tags.append(percent)\n",
    "    return sequence,ner_tags\n",
    "def preprocess(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for line in sentences:\n",
    "        line = line.replace(\"$.\",\"$0.\")\n",
    "        # Apply NER for the line\n",
    "        line,ner_tags = __applyner(line)\n",
    "        tokens = __clean_data(line,ner_tags)\n",
    "        cleaned_sentences.append(tokens)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = preprocess(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedded_tweets(cleaned_tweets):\n",
    "    vec = np.zeros([len(cleaned_tweets),300], dtype = 'float32') \n",
    "    c=0\n",
    "    for i in cleaned_tweets:\n",
    "        for j in i:\n",
    "            try:\n",
    "                j=str(j)\n",
    "                k=embeddings[j]\n",
    "                vec[c]=(vec[c]+np.array(k))\n",
    "            except:\n",
    "                continue\n",
    "        c=c+1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = get_embedded_tweets([cleaned_tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = model_pretrained.predict(x_test)\n",
    "class2 = model_our.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class with pretrained Glove:  [1]\n",
      "Predicted with our Glove:  [1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class with pretrained Glove: \", class1)\n",
    "print(\"Predicted with our Glove: \", class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
