{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import concurrent\n",
    "import contractions\n",
    "import en_core_web_sm\n",
    "import logging as log\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import spacy\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from spacy import displacy\n",
    "from Stemmer import Stemmer\n",
    "from word2number import w2n\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "from word2number import w2n\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickfile = open('D:\\College\\Study\\IRE\\Project\\data\\pretrainedEmbeddings\\embeddings.pickle','rb')\n",
    "embeddings = pickle.load(pickfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(id, info_name, data_json):\n",
    "    return data_json[id]['info'][0][info_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        tweets = []\n",
    "        target_num = []\n",
    "        offset = []\n",
    "        target_cashtag = []\n",
    "        relation = []\n",
    "\n",
    "        data_json = json.load(file)\n",
    "\n",
    "        def getAspects(aspect):\n",
    "            aspect = aspect.replace('[', '')\n",
    "            aspect = aspect.replace(']', '')\n",
    "            aspect = aspect.replace('\\'', '')\n",
    "            return aspect.split('/')\n",
    "\n",
    "        for id in data_json:\n",
    "            tweets.append(id['tweet'].lower())\n",
    "            target_num.append(id['target_num'])\n",
    "            offset.append(id['offset'])\n",
    "            target_cashtag.append(id['target_cashtag'].lower())\n",
    "            relation.append(id['relation'])\n",
    "            \n",
    "\n",
    "    return tweets, target_num, offset, target_cashtag, relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __removePunctuations(sequence,ner_tags):\n",
    "    #########################################################################################\n",
    "    # This method removes any punctuations and gets only the text from the given sequence.\n",
    "    #########################################################################################\n",
    "    try:\n",
    "        if sequence is not None and sequence.strip() != \"\":\n",
    "            if sequence in ner_tags:\n",
    "                return re.sub('[^A-Za-z0-9%$.]+',' ',sequence)\n",
    "            else:\n",
    "                return re.sub('[^A-Za-z0-9$%]+',' ',sequence)\n",
    "        return sequence # return sequence as is without any changes\n",
    "    except:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        err = \"Error occurred while removing punctuations in the sequence '{0}'. Error is: {1}; {2}\".format(sequence, str(exc_type), str(exc_value))\n",
    "        raise Exception(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __clean_data(data,ner_tags):\n",
    "    #########################################################################################\n",
    "    # This method cleans the data by applying following API's and returns list of preprocessed tokens\n",
    "    #     data.lower()\n",
    "    #     __expandContractions(data)\n",
    "    #     __removeHtmlTags(data)\n",
    "    #     __replaceurls(data)\n",
    "    #     __removePunctuations(data)\n",
    "    #     data.split(\" \")\n",
    "    #########################################################################################\n",
    "    #data = __expandContractions(data)\n",
    "    #data = __removeHtmlTags(data) # remove tags\n",
    "    #data = __replaceurls(data)\n",
    "    data = data.replace(\"$\",\"$ \")\n",
    "    data = data.replace(\"%\",\" % \")\n",
    "    data = data.split(\" \")\n",
    "    clean = []\n",
    "    for word in data:\n",
    "        w_clean = __removePunctuations(word,ner_tags)\n",
    "        clean.extend(w_clean.split(\" \"))\n",
    "    data = clean\n",
    "    data = [word if word.isupper() and word.lower() in ner_tags else word.lower() for word in data]\n",
    "    dollar_list = ['$','k','%']\n",
    "    data = [d for d in data if len(d)>=2 or d in dollar_list]\n",
    "#     clean_list.append(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedded_tweets(cleaned_tweets):\n",
    "    vec = np.zeros([len(cleaned_tweets),300], dtype = 'float32') \n",
    "    c=0\n",
    "    for i in cleaned_tweets:\n",
    "        for j in i:\n",
    "            try:\n",
    "                j=str(j)\n",
    "                k=embeddings[j]\n",
    "                vec[c]=(vec[c]+np.array(k))\n",
    "            except:\n",
    "                continue\n",
    "        c=c+1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __applyner(sequence):\n",
    "    #########################################################################################\n",
    "    # This method applies NER and returns the sequence according to the operation performed based on NER tag.\n",
    "    #########################################################################################\n",
    "    pickfile = open('tickermapping.pickle','rb')\n",
    "    tickermapping = pickle.load(pickfile)\n",
    "    ner_tags = []\n",
    "    doc = nlp(sequence)  # applying NER\n",
    "    for X in doc.ents:\n",
    "        # If the NER class is ORG\n",
    "        if X.label_ == 'ORG':\n",
    "            \"X.text can take microsoft corp or abcd name MSFT\"\n",
    "            text = X.text\n",
    "            if text in tickermapping.keys():\n",
    "                text = tickermapping[X.text]\n",
    "            text = re.sub(r'[^\\w\\s]', '', X.text).lower()\n",
    "            if 'inc' in text:\n",
    "                text = text.replace('inc', '')\n",
    "            if 'ltd' in text:\n",
    "                text = text.replace('ltd', '')\n",
    "            if 'llp' in text:\n",
    "                text = text.replace('llp', '')\n",
    "            if 'limited' in text:\n",
    "                text = text.replace('limited', '')\n",
    "            if 'corp' in text:\n",
    "                text = text.replace('corp', '')\n",
    "            if 'the' in text.lower():\n",
    "                text = text.replace('the','')\n",
    "            sequence = sequence.replace(X.text, text)\n",
    "            ner_tags.extend(text.lower().split(\" \"))\n",
    "        # If NER class is MONEY\n",
    "        if X.label_ == 'MONEY':\n",
    "            new_X = X.text.lower()\n",
    "            if 'approximately' in new_X:  # Remove all the words which might appear in NER money class\n",
    "                new_X = new_X.replace('approximately', '')\n",
    "            if 'per' in new_X:\n",
    "                new_X = new_X.replace('per', '')\n",
    "            if 'to' in new_X:\n",
    "                new_X = new_X.replace('to', '')\n",
    "            if 'and' in new_X:\n",
    "                new_X = new_X.replace('and', '')\n",
    "            if 'between' in new_X:\n",
    "                new_X = new_X.replace('between', '')\n",
    "            if 'phone' in new_X:\n",
    "                continue\n",
    "            # Apply NER for the string which is obtained after removing other words this gives $200, $500 as separate ones\n",
    "            if '$' not in new_X:\n",
    "                new_X = \"$\"+new_X\n",
    "            doc1 = nlp(new_X)\n",
    "            for Y in doc1.ents:\n",
    "                money = Y.text\n",
    "                if ' ' not in money:\n",
    "                    act_money = money.replace(',', '')  # Actual Money\n",
    "                    #act_money = act_money.replace('.','')\n",
    "                    sequence = sequence.replace(Y.text, act_money)  # Replace original money text with actual money\n",
    "                    ner_tags.append(act_money)\n",
    "                    # print(act_money)\n",
    "                else:\n",
    "                    money = Y.text[Y.text.find(\"$\") + 1:]\n",
    "                    k = money.find(' ')\n",
    "                    try:\n",
    "                        act_money = float(money[:k].replace(',', ''))\n",
    "                        #act_money = act_money.replace('.','')\n",
    "                        money_conv = w2n.word_to_num(money[k:])  # Conversion of word types million to *1e6\n",
    "                        sequence = sequence.replace(Y.text, \"$ \"+str(act_money * money_conv))  # Replace original money text with actual money\n",
    "                        #print(\"Converted from\", money, act_money * money_conv)\n",
    "                    except:\n",
    "                        continue  # if any exception dont modify the original sentence and continue\n",
    "        # If NER class is LAW\n",
    "        if X.label_ == 'LAW':\n",
    "            new_X = X.text\n",
    "            new_X = re.sub(r'[\\d.!?\\-\"]', '', new_X)\n",
    "            if 'the' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('the', '')\n",
    "            if 'of' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('of', '')\n",
    "            if 'section' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('section', '')\n",
    "            sequence = sequence.replace(X.text, new_X)\n",
    "            ner_tags.extend(new_X.split(\" \"))\n",
    "        # If NER class is Location\n",
    "        if X.label_ == 'GPE':\n",
    "            new_X = X.text.lower()\n",
    "            new_X = re.sub(r'[\\d.!?\\-\"]', '', new_X)\n",
    "            if 'the' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('the', '')\n",
    "            if '.' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('.', '')\n",
    "            sequence = sequence.replace(X.text, new_X)\n",
    "            ner_tags.extend(new_X.split(\" \"))\n",
    "        # If NER class is Person\n",
    "        if X.label_ == 'PERSON':\n",
    "            new_X = X.text.lower()\n",
    "            new_X = re.sub(r'[\\d.!?\\-\"]', '', new_X)\n",
    "            if 'the' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('the', '')\n",
    "            if '.' in new_X.lower():\n",
    "                new_X = new_X.lower().replace('.', '')\n",
    "            sequence = sequence.replace(X.text, new_X)\n",
    "            ner_tags.extend(new_X.split(\" \"))\n",
    "        if X.label_ == 'CARDINAL':\n",
    "            number = X.text\n",
    "            number = number.replace(',','')\n",
    "            #number = number.replace('.','')\n",
    "            if number.isnumeric():\n",
    "                sequence = sequence.replace(X.text, number)\n",
    "        if X.label_ == 'QUANTITY':\n",
    "            quantity = X.text.split(\" \")\n",
    "            for number in quantity:\n",
    "                number = number.replace(',','')\n",
    "                number = number.replace('.','')\n",
    "                if number.isnumeric():\n",
    "                    sequence = sequence.replace(X.text, number)\n",
    "        if X.label_ == \"PERCENT\":\n",
    "            percent = X.text.replace('%','')\n",
    "            ner_tags.append(percent)\n",
    "    return sequence,ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for line in sentences:\n",
    "        line = line.replace(\"$.\",\"$0.\")\n",
    "        # Apply NER for the line\n",
    "        line,ner_tags = __applyner(line)\n",
    "        tokens = __clean_data(line,ner_tags)\n",
    "        cleaned_sentences.append(tokens)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, target_num, offset, target_cashtag, y_train = load_data(r\"D:\\College\\Study\\IRE\\Project\\data\\Extrinsic Task 2 (Binary Classification) Data\\FinNum_training_v3.json\")\n",
    "cleaned_tweets_train = preprocess(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$xxii scott gottlieb, commissioner of fda speech transcript from november 3rd, less than 2 months left in year then.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, target_num, offset, target_cashtag, y_test = load_data(r\"D:\\College\\Study\\IRE\\Project\\data\\Extrinsic Task 2 (Binary Classification) Data\\FinNum_dev_v3.json\")\n",
    "cleaned_tweets_test = preprocess(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_embedded_tweets(cleaned_tweets_train)\n",
    "X_test = get_embedded_tweets(cleaned_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy= 83.23754789272031 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.10      0.18       194\n",
      "           1       0.83      1.00      0.91       850\n",
      "\n",
      "    accuracy                           0.83      1044\n",
      "   macro avg       0.91      0.55      0.54      1044\n",
      "weighted avg       0.86      0.83      0.77      1044\n",
      "\n",
      "\n",
      "\n",
      "Confusion_Matrix\n",
      "[[ 19 175]\n",
      " [  0 850]]\n",
      "F1:  0.9066666666666667\n"
     ]
    }
   ],
   "source": [
    "clf=svm.SVC(C=0.3)\n",
    "clf.fit(X_train,y_train)\n",
    "prediction=clf.predict(X_test)\n",
    "\n",
    "pred_labels=prediction==y_test\n",
    "acc=0.0\n",
    "for i in pred_labels:\n",
    "    if i==True:\n",
    "        acc+=1\n",
    "        \n",
    "print ('SVM accuracy=',(acc)/len(pred_labels)*100,'%')\n",
    "print(classification_report(y_test,prediction,labels=np.unique(y_test)))\n",
    "print(\"\\n\\nConfusion_Matrix\")\n",
    "print(confusion_matrix(y_test,prediction))\n",
    "print(\"F1: \",f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy= 85.727969348659 %\n",
      "F1 score:  0.9189777052746058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rlf = RandomForestClassifier()\n",
    "rlf.fit(X_train,y_train)\n",
    "prediction=rlf.predict(X_test)\n",
    "pred_labels=prediction==y_test\n",
    "acc=0.0\n",
    "for i in pred_labels:\n",
    "    if i==True:\n",
    "        acc+=1\n",
    "print ('Random forest accuracy=',(acc)/len(pred_labels)*100,'%')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# print(classification_report(y_test,prediction,labels=np.unique(y_test)))\n",
    "# print(\"\\n\\nConfusion_Matrix\")\n",
    "# print(confusion_matrix(y_test,prediction))\n",
    "print(\"F1 score: \",f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy= 86.30268199233716 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.27      0.42       194\n",
      "           1       0.86      1.00      0.92       850\n",
      "\n",
      "    accuracy                           0.86      1044\n",
      "   macro avg       0.92      0.63      0.67      1044\n",
      "weighted avg       0.88      0.86      0.83      1044\n",
      "\n",
      "\n",
      "\n",
      "Confusion_Matrix\n",
      "[[ 52 142]\n",
      " [  1 849]]\n",
      "F1:  0.9223248234655079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "cl=GradientBoostingClassifier()\n",
    "cl.fit(X_train,y_train)\n",
    "prediction=cl.predict(X_test)\n",
    "pred_labels=prediction==y_test\n",
    "acc=0.0\n",
    "for i in pred_labels:\n",
    "    if i==True:\n",
    "        acc+=1\n",
    "print ('SVM accuracy=',(acc)/len(pred_labels)*100,'%')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,prediction,labels=np.unique(y_test)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"\\n\\nConfusion_Matrix\")\n",
    "print(confusion_matrix(y_test,prediction))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1: \",f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(clf, open('model_pretrained_classification_svc.pickle', 'wb'))\n",
    "#pickle.dump(clf, open('model_our_classification_svc.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(rlf, open('model_pretrained_classification_rforest.pickle', 'wb'))\n",
    "#pickle.dump(rlf, open('model_our_classification_rforest.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(cl, open('model_pretrained_classification_gboost.pickle', 'wb'))\n",
    "#pickle.dump(cl, open('model_our_classification_gboost.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
